{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/EduProgrammingSimul/EduProgramRepo/blob/main/TrialNotebookProject.ipynb",
      "authorship_tag": "ABX9TyP+luB0xjusIMilzmmHs7KV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduProgrammingSimul/EduProgramRepo/blob/main/TrialNotebookProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iiLPgpyvqG5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "import gymnasium as gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as timer # Avoid conflict with time variable in loops\n",
        "import os\n",
        "import argparse # Import argparse\n",
        "import sys # For exiting cleanly\n",
        "\n",
        "# Register the custom environment\n",
        "from gymnasium.envs.registration import register\n",
        "register(\n",
        "    id='PWREnv-v0',\n",
        "    entry_point='envs.pwr_env:PWREnv',\n",
        ")\n",
        "\n",
        "# Import necessary components AFTER registration\n",
        "from config import * # Import configurations like DEFAULT_SIMULATION_TIME\n",
        "from controllers.pid import PIDController\n",
        "from controllers.flc import FLCController\n",
        "from controllers.mpc import MPCController # Placeholder\n",
        "from controllers.rl.agent import setup_rl_agent, train_rl_agent, evaluate_rl_agent\n",
        "# Import only needed function if scenario list isn't required globally\n",
        "from scenarios.definitions import get_scenario_profile\n",
        "from utils.evaluation import calculate_performance_metrics\n",
        "from utils.plotting import plot_simulation_results\n",
        "# Removed plot_comparison_metrics as we run single simulation\n",
        "\n",
        "# --- Helper Function (remains the same) ---\n",
        "def run_simulation(env_id, controller_type, scenario_name, sim_time, rl_model=None):\n",
        "    \"\"\"Runs a single simulation episode.\"\"\"\n",
        "    print(f\"\\n--- Running Scenario: {scenario_name} with Controller: {controller_type} ---\")\n",
        "\n",
        "    # Create environment instance for this run\n",
        "    env = gym.make(env_id, scenario_name=scenario_name, simulation_time=sim_time, render_mode=None) # No render during batch runs\n",
        "\n",
        "    # Initialize controller for this run\n",
        "    controller = None\n",
        "    try:\n",
        "        initial_power_fraction = env.unwrapped.initial_power_fraction\n",
        "        initial_valve = initial_power_fraction\n",
        "        print(f\"Debug: Accessed initial_power_fraction: {initial_power_fraction:.2f}\")\n",
        "    except AttributeError:\n",
        "         print(\"Warning: Could not get initial_power_fraction from unwrapped env. Using default.\")\n",
        "         initial_valve = 0.9 # Default fallback\n",
        "\n",
        "    if controller_type == 'PID':\n",
        "        controller = PIDController(setpoint=NOMINAL_SPEED_RPM)\n",
        "        controller.reset()\n",
        "    elif controller_type == 'FLC':\n",
        "        controller = FLCController(setpoint=NOMINAL_SPEED_RPM)\n",
        "        controller.reset(initial_valve_pos=initial_valve)\n",
        "    elif controller_type == 'MPC':\n",
        "        controller = MPCController() # Placeholder\n",
        "        controller.reset(initial_valve_pos=initial_valve)\n",
        "    elif controller_type == 'RL':\n",
        "        if rl_model is None:\n",
        "            raise ValueError(\"RL model must be provided for RL controller type evaluation.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown controller type: {controller_type}\")\n",
        "\n",
        "    # Simulation Loop\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    history = []\n",
        "    start_time_exec = timer.time()\n",
        "\n",
        "    while not done:\n",
        "        current_measurement = info['speed_rpm']\n",
        "        current_valve_pos = info['valve_position']\n",
        "        current_step = env.unwrapped.current_step\n",
        "\n",
        "        # Sensor Fault Injection placeholder (can be refined)\n",
        "        if scenario_name == 'sensor_fail':\n",
        "             fault_start_time = 20\n",
        "             fault_duration = 30\n",
        "             if fault_start_time <= env.unwrapped.current_time < fault_start_time + fault_duration:\n",
        "                  current_measurement = NOMINAL_SPEED_RPM\n",
        "\n",
        "        # Get action from controller or RL agent\n",
        "        if controller_type == 'RL':\n",
        "            action, _states = rl_model.predict(obs, deterministic=True)\n",
        "        elif controller_type == 'PID':\n",
        "            action = controller.calculate_action(current_measurement)\n",
        "        elif controller_type == 'FLC':\n",
        "            action = controller.calculate_action(current_measurement)\n",
        "        elif controller_type == 'MPC':\n",
        "            action = controller.calculate_action(current_measurement, current_step, current_valve_pos)\n",
        "        else: # Should not happen due to argparse choices\n",
        "             action = env.action_space.sample()\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        history.append(info)\n",
        "\n",
        "    end_time_exec = timer.time()\n",
        "    exec_time = end_time_exec - start_time_exec\n",
        "    print(f\"--- Simulation Complete ({exec_time:.2f}s execution time) ---\")\n",
        "    env.close()\n",
        "\n",
        "    history_df = pd.DataFrame(history)\n",
        "    return history_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Argument Parsing ---\n",
        "    parser = argparse.ArgumentParser(description=\"Run PWR Governor Control Simulation\")\n",
        "    parser.add_argument(\n",
        "        \"-c\", \"--controller\",\n",
        "        required=True,\n",
        "        choices=['PID', 'FLC', 'MPC', 'RL'],\n",
        "        help=\"Type of controller to use.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-s\", \"--scenario\",\n",
        "        default='stable',\n",
        "        # TODO: Dynamically list available scenarios if possible, or list manually\n",
        "        help=\"Name of the scenario to run (e.g., 'stable', 'gradual_increase', 'sudden_load_increase', etc.). Default: 'stable'.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-t\", \"--time\",\n",
        "        type=int,\n",
        "        default=DEFAULT_SIMULATION_TIME,\n",
        "        help=f\"Simulation duration in seconds. Default: {DEFAULT_SIMULATION_TIME}s.\"\n",
        "    )\n",
        "    # RL specific arguments\n",
        "    parser.add_argument(\n",
        "        \"--train\",\n",
        "        action='store_true',\n",
        "        help=\"Train a new RL model (only applies if --controller RL).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--timesteps\",\n",
        "        type=int,\n",
        "        default=150000, # Default training timesteps\n",
        "        help=\"Total timesteps for RL training (only applies with --train).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_path\",\n",
        "        default=None, # Default to constructing path later\n",
        "        help=\"Path to load a pre-trained RL model (only applies if --controller RL and not --train).\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Configuration from args ---\n",
        "    ENV_ID = 'PWREnv-v0'\n",
        "    SELECTED_CONTROLLER = args.controller\n",
        "    SELECTED_SCENARIO = args.scenario\n",
        "    SIMULATION_TIME = args.time\n",
        "    RUN_RL_TRAINING = args.train\n",
        "    RL_TOTAL_TIMESTEPS = args.timesteps\n",
        "    PROVIDED_RL_MODEL_PATH = args.model_path\n",
        "\n",
        "    # Construct default RL paths if not provided\n",
        "    RL_MODEL_DIR = \"./rl_models/\"\n",
        "    RL_MODEL_NAME = \"pwr_sac_model_final\" # Default name\n",
        "    RL_LOG_DIR = \"./rl_logs/\"\n",
        "    DEFAULT_RL_MODEL_PATH = os.path.join(RL_MODEL_DIR, f\"{RL_MODEL_NAME}.zip\")\n",
        "    RL_MODEL_PATH_TO_USE = PROVIDED_RL_MODEL_PATH if PROVIDED_RL_MODEL_PATH else DEFAULT_RL_MODEL_PATH\n",
        "\n",
        "    # --- RL Agent Training / Loading ---\n",
        "    rl_agent = None\n",
        "    if SELECTED_CONTROLLER == 'RL':\n",
        "        os.makedirs(RL_MODEL_DIR, exist_ok=True)\n",
        "        os.makedirs(RL_LOG_DIR, exist_ok=True)\n",
        "\n",
        "        if RUN_RL_TRAINING:\n",
        "            print(f\"--- Preparing for RL Training ({RL_TOTAL_TIMESTEPS} timesteps) ---\")\n",
        "            try:\n",
        "                # Setup agent for training\n",
        "                rl_agent_train, vec_env_for_train = setup_rl_agent(\n",
        "                    ENV_ID, log_dir=RL_LOG_DIR, train=True\n",
        "                )\n",
        "                # Train the agent\n",
        "                train_rl_agent(\n",
        "                    rl_agent_train,\n",
        "                    total_timesteps=RL_TOTAL_TIMESTEPS,\n",
        "                    save_path=os.path.join(RL_MODEL_DIR, RL_MODEL_NAME) # Save with default name\n",
        "                )\n",
        "                print(\"--- Training complete. Exiting. ---\")\n",
        "                sys.exit(0) # Exit after training\n",
        "            except Exception as e:\n",
        "                print(f\"!!! ERROR during RL training setup or execution !!!\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                sys.exit(1) # Exit with error code\n",
        "\n",
        "        else: # Load for evaluation\n",
        "            print(f\"--- Loading RL Agent for Evaluation ---\")\n",
        "            try:\n",
        "                rl_agent, _ = setup_rl_agent( # We don't need the vec_env here\n",
        "                    ENV_ID, train=False, model_path=RL_MODEL_PATH_TO_USE\n",
        "                )\n",
        "                print(f\"RL model loaded successfully from {RL_MODEL_PATH_TO_USE}\")\n",
        "            except FileNotFoundError as e:\n",
        "                print(f\"!!! ERROR: RL model file not found at {RL_MODEL_PATH_TO_USE} !!!\")\n",
        "                print(\"Cannot run RL controller evaluation.\")\n",
        "                sys.exit(1) # Exit if model not found for eval\n",
        "            except Exception as e:\n",
        "                print(f\"!!! ERROR loading RL model !!!\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                sys.exit(1) # Exit with error code\n",
        "\n",
        "    elif RUN_RL_TRAINING:\n",
        "        print(\"Warning: --train flag provided but controller is not RL. Training flag ignored.\")\n",
        "\n",
        "\n",
        "    # --- Run the Single Simulation ---\n",
        "    print(f\"\\n=============================================\")\n",
        "    print(f\" Starting Simulation:\")\n",
        "    print(f\" Controller: {SELECTED_CONTROLLER}\")\n",
        "    print(f\" Scenario:   {SELECTED_SCENARIO}\")\n",
        "    print(f\" Duration:   {SIMULATION_TIME}s\")\n",
        "    print(f\"=============================================\")\n",
        "\n",
        "    try:\n",
        "        history_df = run_simulation(\n",
        "            env_id=ENV_ID,\n",
        "            controller_type=SELECTED_CONTROLLER,\n",
        "            scenario_name=SELECTED_SCENARIO,\n",
        "            sim_time=SIMULATION_TIME,\n",
        "            rl_model=rl_agent # Pass agent if loaded (will be None otherwise)\n",
        "        )\n",
        "\n",
        "        # --- Process and Plot Results ---\n",
        "        metrics = calculate_performance_metrics(history_df)\n",
        "        print(\"\\n--- Performance Metrics ---\")\n",
        "        # Pretty print metrics\n",
        "        max_key_len = max(len(k) for k in metrics.keys())\n",
        "        for key, value in metrics.items():\n",
        "             print(f\"{key:<{max_key_len}} : {value:.4f}\")\n",
        "\n",
        "        # Save metrics to a file specific to this run\n",
        "        metrics_filename = f\"metrics_{SELECTED_SCENARIO}_{SELECTED_CONTROLLER}.csv\"\n",
        "        metrics_series = pd.Series(metrics)\n",
        "        metrics_series.to_csv(metrics_filename)\n",
        "        print(f\"\\nMetrics saved to {metrics_filename}\")\n",
        "\n",
        "        # Save history to a file specific to this run\n",
        "        history_filename = f\"history_{SELECTED_SCENARIO}_{SELECTED_CONTROLLER}.csv\"\n",
        "        history_df.to_csv(history_filename, index=False)\n",
        "        print(f\"Full history saved to {history_filename}\")\n",
        "\n",
        "\n",
        "        plot_simulation_results(\n",
        "            history_df,\n",
        "            title=f\"Scenario: {SELECTED_SCENARIO} - Controller: {SELECTED_CONTROLLER}\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! ERROR during simulation run for {SELECTED_SCENARIO} with {SELECTED_CONTROLLER} !!!\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- Simulation Finished Successfully ---\")\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "AXGb1Jk1umEM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}